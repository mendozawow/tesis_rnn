@article{Garcia2014,
abstract = {Abstract The results of botnet detection methods are usually presented without any comparison. Although it is generally accepted that more comparisons with third-party methods may help to improve the area, few papers could do it. Among the factors that prevent a comparison are the difficulties to share a dataset, the lack of a good dataset, the absence of a proper description of the methods and the lack of a comparison methodology. This paper compares the output of three different botnet detection methods by executing them over a new, real, labeled and large botnet dataset. This dataset includes botnet, normal and background traffic. The results of our two methods (BClus and CAMNEP) and BotHunter were compared using a methodology and a novel error metric designed for botnet detections methods. We conclude that comparing methods indeed helps to better estimate how good the methods are, to improve the algorithms, to build better datasets and to build a comparison methodology.},
author = {Garc{\'{\i}}a, Sebasti{\'{a}}n and Grill, Martin and Stiborek, Jan and Zunino, Alejandro},
doi = {http://dx.doi.org/10.1016/j.cose.2014.05.011},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{\i}}a et al. - 2014 - An Empirical Comparison of Botnet Detection Methods.pdf:pdf},
issn = {0167-4048},
journal = {Computers {\&} Security},
keywords = {Anomaly detection,Botnet dataset,Botnet detection,Malware detection,Methods comparison,Network traffic},
number = {0},
pages = {100 -- 123},
title = {{An Empirical Comparison of Botnet Detection Methods}},
url = {http://www.sciencedirect.com/science/article/pii/S0167404814000923},
volume = {45},
year = {2014}
}
@article{,
doi = {10.13140/2.1.3488.8006},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Identifying , Modeling and Detecting Botnet Behaviors in the Network U niversidad N acional del C entro de la P rovinci.pdf:pdf},
number = {August},
title = {{Identifying , Modeling and Detecting Botnet Behaviors in the Network U niversidad N acional del C entro de la P rovincia de B uenos A ires D octoral T hesis Identifying , Modeling and Detecting Botnet Behaviors in the Network}},
year = {2015}
}
@article{Karim2014,
author = {Karim, Ahmad and Salleh, Rosli Bin and Shiraz, Muhammad and Shah, Syed Adeel Ali and Awan, Irfan and Anuar, Nor Badrul},
doi = {10.1631/jzus.C1300242},
file = {:home/h2o/rnn/tesis/documents/detecting{\_}botnets.pdf:pdf},
issn = {1869-1951},
journal = {Journal of Zhejiang University SCIENCE C},
keywords = {10,1631,anomaly detection,attack,botnet detection,c1300242,defense,doi,jzus,network security,taxonomy},
number = {11},
pages = {943--983},
title = {{Botnet detection techniques: review, future trends, and issues}},
url = {http://link.springer.com/10.1631/jzus.C1300242},
volume = {15},
year = {2014}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Livadas2006,
abstract = {To date, techniques to counter cyber-attacks have predominantly been reactive; they focus on monitoring network traffic, detecting anomalies and cyber-attack traffic patterns, and, a posteriori, combating the cyber-attacks and mitigating their effects. Contrary to such approaches, we advocate proactively detecting and identifying botnets prior to their being used as part of a cyber-attack (Strayer et al., 2006). In this paper, we present our work on using machine learning-based classification techniques to identify the command and control (C2) traffic of IRC-based botnets - compromised hosts that are collectively commanded using Internet relay chat (IRC). We split this task into two stages: (I) distinguishing between IRC and non-IRC traffic, and (II) distinguishing between botnet and real IRC traffic. For stage I, we compare the performance of J48, naive Bayes, and Bayesian network classifiers, identify the features that achieve good overall classification accuracy, and determine the classification sensitivity to the training set size. While sensitive to the training data and the attributes used to characterize communication flows, machine learning-based classifiers show promise in identifying IRC traffic. Using classification in stage II is trickier, since accurately labeling IRC traffic as botnet and non-botnet is challenging. We are currently exploring labeling flows as suspicious and non-suspicious based on telltales of hosts being compromised},
author = {Livadas, C. and Walsh, R. and Lapsley, D. and Strayer, W.T.},
doi = {10.1109/LCN.2006.322210},
isbn = {1-4244-0418-5},
issn = {0742-1303},
journal = {Proceedings. 2006 31st IEEE Conference on Local Computer Networks},
title = {{Usilng Machine Learning Technliques to Identify Botnet Traffic}},
year = {2006}
}
@article{Chawla,
author = {Chawla, Nitesh V},
title = {{DATA MINING FOR IMBALANCED DATASETS : AN OVERVIEW}}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
pmid = {18190633},
title = {{Comparison of balancing techniques for unbalanced datasets}},
volume = {16},
year = {2002}
}
@article{Garcia2014c,
annote = {Articulo que describe el comportamiento de C{\&}amp;C},
author = {Garc{\'{\i}}a, Sebasti{\'{a}}n and Uhl{\'{\i}}r, Vojt{\v{e}}ch and Rehak, Martin},
doi = {10.1145/2602945.2602949},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{\i}}a, Uhl{\'{\i}}Å™, Rehak - 2014 - Identifying and modeling botnet C{\&}C behaviors.pdf:pdf},
isbn = {9781450327282},
journal = {Proceedings of the 1st International Workshop on Agents and CyberSecurity - ACySE '14},
keywords = {botnet,malware,network behavior,network security},
number = {MAY 2014},
pages = {1--8},
title = {{Identifying and modeling botnet C{\&}C behaviors}},
url = {http://dl.acm.org/citation.cfm?doid=2602945.2602949},
year = {2014}
}
@article{Hachem2011,
abstract = {The new threat of the Internet, but little known to the 'general public' is constituted by botnets. Botnets are networks of infected computers, which are headed by a pirate called also 'Attacker' or 'Master'. The botnets are nowadays mainly responsible for large-scale coordinated attacks. The attacker can ask the infected computers called 'Agents' or 'Zombies' to perform all sorts of tasks for him, like sending spam, performing DDoS attacks, phishing campaigns, delivering malware, or leasing or selling their botnets to other fraudsters anywhere. In this paper we present a classification that reflects the life cycle and current resilience techniques of botnets, distinguishing the propagation, the injection, the control and the attack phases. Then we study the effectiveness of the adopted taxonomy by applying it to existing botnets to study their main characteristics. We conclude by the upcoming steps in our research.},
author = {Hachem, Nabil and {Ben Mustapha}, Yosra and Granadillo, Gustavo Gonzalez and Debar, Herve},
doi = {10.1109/SAR-SSI.2011.5931395},
file = {:home/h2o/rnn/tesis/documents/botnets/Botnets{\_}Lifecycle{\_}and{\_}taxonomy.pdf:pdf},
isbn = {9781457707377},
journal = {2011 Conference on Network and Information Systems Security, SAR-SSI 2011, Proceedings},
number = {March 2016},
title = {{Botnets: Lifecycle and taxonomy}},
year = {2011}
}
@article{Saad2011,
abstract = {Botnets have become one of the major threats on the Internet for serving as a vector for carrying attacks against organizations and committing cybercrimes. They are used to generate spam, carry out DDOS attacks and click-fraud, and steal sensitive information. In this paper, we propose a new approach for characterizing and detecting botnets using network traffic behaviors. Our approach focuses on detecting the bots before they launch their attack. We focus in this paper on detecting P2P bots, which represent the newest and most challenging types of botnets currently available. We study the ability of five different commonly used machine learning techniques to meet online botnet detection requirements, namely adaptability, novelty detection, and early detection. The results of our experimental evaluation based on existing datasets show that it is possible to detect effectively botnets during the botnet Command-and- Control (C{\&}amp;C) phase and before they launch their attacks using traffic behaviors only. However, none of the studied techniques can address all the above requirements at once.},
author = {Saad, Sherif and Traore, Issa and Ghorbani, Ali and Sayed, Bassam and Zhao, David and Lu, Wei and Felix, John and Hakimian, Payman},
doi = {10.1109/PST.2011.5971980},
file = {:home/h2o/rnn/tesis/documents/botnets/detecting botnets through network behavior analysis machine learning.pdf:pdf},
isbn = {9781457705847},
journal = {2011 9th Annual International Conference on Privacy, Security and Trust, PST 2011},
number = {March 2016},
pages = {174--180},
title = {{Detecting P2P botnets through network behavior analysis and machine learning}},
year = {2011}
}
@article{Barford2007,
abstract = {The continued growth and diversification of the Internet has been accompanied by an increasing prevalence of attacks and intrusions [40]. It can be argued, however, that a significant change in motivation for malicious activity has taken place over the past several years: from vandalism and recognition in the hacker community, to attacks and intrusions for financial gain. This shift has been marked by a growing sophistication in the tools and methods used to conduct attacks, thereby escalating the network security arms race. Our thesis is that the reactive methods for network security that are predominant today are ultimately insufficient and that more proactive methods are required. One such approach is to develop a foundational understanding of the mechanisms employed by malicious software (malware) which is often readily available in source form on the Internet. While it is well known that large IT security companies maintain detailed databases of this information, these are not openly available and we are not aware of any such open repository. In this chapter we begin the process of codifying the capabilities of malware by dissecting four widely-used Internet Relay Chat (IRC) botnet codebases. Each codebase is classified along seven key dimensions including botnet control mechanisms, host control mechanisms, propagation mechanisms, exploits, delivery mechanisms, obfuscation and deception mechanisms. Our study reveals the complexity of botnet software, and we discusses implications for defense strategies based on our analysis.},
author = {Barford, Paul and Yegneswaran, Vinod},
doi = {10.1007/978-0-387-44599-1},
file = {:home/h2o/rnn/tesis/documents/botnets/inside look at botnets.pdf:pdf},
isbn = {978-0-387-32720-4},
issn = {03601315},
journal = {Malware Detection},
pages = {171--191},
pmid = {8514563},
title = {{An inside look at Botnets}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-44599-1},
volume = {27},
year = {2007}
}
@article{Chang,
author = {Chang, Wentao},
file = {:home/h2o/rnn/tesis/documents/botnets/measuring botnets in the wild.pdf:pdf},
isbn = {9781450332453},
pages = {3--8},
title = {{Measuring Botnets in the Wild : Some New Trends}}
}
@article{Soltani2014,
author = {Soltani, Somayeh and Seno, SAH},
file = {:home/h2o/rnn/tesis/documents/botnets/survey on real world botnets and detection mechanisms.pdf:pdf},
journal = {International Journal of {\ldots}},
number = {2},
pages = {116--127},
title = {{A survey on real world botnets and detection mechanisms}},
url = {http://iaesjournal.com/online/index.php/IJINS/article/view/6231},
volume = {3},
year = {2014}
}
@book{Olson2008,
abstract = {The intent of this book is to describe some recent data mining tools that have proven effective in dealing with data sets which often involve uncer- tain description or other complexities that cause difficulty for the conven- tional approaches of logistic regression, neural network models, and deci- sion trees. Among these traditional algorithms, neural network models often have a relative advantage when data is complex. We will discuss methods with simple examples, review applications, and evaluate relative advantages of several contemporary methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Olson, David L Dl and Delen, Dursun},
doi = {10.1007/978-3-540-76917-0},
eprint = {arXiv:1011.1669v3},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/Advanced-Data-Mining-Techniques.pdf:pdf},
isbn = {9783540769163},
issn = {00093696},
pages = {2008},
pmid = {25246403},
title = {{Advanced data mining techniques}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=2vb-LZEn8uUC{\&}oi=fnd{\&}pg=PA2{\&}dq=Advanced+Data+Mining+Techniques{\&}ots=zV10227KpP{\&}sig=dlD9enPYAgM0VL1repniEvrO-JU$\backslash$nhttp://www.springerlink.com/index/10.1007/978-3-540-76917-0},
year = {2008}
}
@article{Medsker2000,
abstract = {With existent uses ranging from motion detection to music synthesis to financial forecasting, recurrent neural networks have generated widespread attention. The tremendous interest in these networks drives Recurrent Neural Networks: Design and Applications, a summary of the design, applications, current research, and challenges of this subfield of artificial neural networks. This overview incorporates every aspect of recurrent neural networks. It outlines the wide variety of complex learning techniques and associated research projects. Each chapter addresses architectures, from fully connected to partially connected, including recurrent multilayer feedforward. It presents problems involving trajectories, control systems, and robotics, as well as RNN use in chaotic systems. The authors also share their expert knowledge of ideas for alternate designs and advances in theoretical aspects. The dynamical behavior of recurrent neural networks is useful for solving problems in science, engineering, and business. This approach will yield huge advances in the coming years. Recurrent Neural Networks illuminates the opportunities and provides you with a broad view of the current events in this rich field.},
author = {Medsker, L. R. and Jain, L. C.},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/RNN{\_}Design{\_}And{\_}Applications{\_}LR{\_}Medsker.pdf:pdf},
isbn = {9780849371813},
pages = {414},
title = {{Recurrent neural networks: design and applications}},
url = {http://books.google.co.uk/books?id=ME1SAkN0PyMC},
year = {2000}
}
@article{Boden2001,
author = {Bod{\'{e}}n, Mikael},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/10.1.1.16.6652.pdf:pdf},
number = {2},
pages = {1--10},
title = {{A Guide to Recurrent Neural Networks and Backpropagation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.6652},
year = {2001}
}
@article{Golik2013,
abstract = {In this paper we investigate the error criteria that are optimized during the training of artificial neural networks (ANN). We compare the bounds of the squared error (SE) and the crossentropy (CE) criteria being the most popular choices in stateof- The art implementations. The evaluation is performed on automatic speech recognition (ASR) and handwriting recognition (HWR) tasks using a hybrid HMM-ANN model. We find that with randomly initialized weights, the squared error based ANN does not converge to a good local optimum. However, with a good initialization by pre-training, the word error rate of our best CE trained system could be reduced from 30.9{\%} to 30.5{\%} on the ASR, and from 22.7{\%} to 21.9{\%} on the HWR task by performing a few additional "fine-tuning" iterations with the SE criterion. Copyright Â© 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01842v1},
author = {Golik, Pavel and Doetsch, Patrick and Ney, Hermann},
doi = {10.1145/1102351.1102422},
eprint = {arXiv:1503.01842v1},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/11paper.pdf:pdf},
isbn = {1595931805},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Automatic speech recognition,Handwriting recognition,Hybrid approach,Training criterion for ANN training},
number = {2},
pages = {1756--1760},
title = {{Cross-entropy vs. Squared error training: A theoretical and experimental comparison}},
volume = {2},
year = {2013}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/1308.0850v5.pdf:pdf},
isbn = {2000201075},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1{\_}53},
eprint = {arXiv:1506.02078v1},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/1506.02078v2.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}},
year = {2015}
}
@article{Jozefowicz2015,
author = {Jozefowicz, R},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/jozefowicz15.pdf:pdf},
journal = {Proceedings of the  {\ldots}},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2015{\_}jozefowicz15.pdf},
volume = {37},
year = {2015}
}
@article{Neural2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2329v5},
author = {Neural, Recurrent and Architectures, Network and Models, State Space and Time, Backpropagation Through and Memory, Associative and Networks, Hopfield and Machines, Boltzmann},
doi = {10.1201/9781420049176},
eprint = {arXiv:1409.2329v5},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/l12.pdf:pdf},
isbn = {0849371813},
pages = {1--20},
title = {{Recurrent Neural Networks}},
year = {2015}
}
@article{Martens2011,
abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely diffi- cult to train them properly. Fortunately, re- cent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence prob- lems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free op- timizer (HF) by applying them to character-level language modeling tasks. The standard RNN ar- chitecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or gated) con- nections which allow the current input charac- ter to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character- level language modeling a hierarchical non- parametric sequence model. To our knowledge this represents the largest recurrent neural net- work application to date.},
author = {Martens, James},
doi = {2},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/LANG-RNN.pdf:pdf},
isbn = {9781450306195},
issn = {1},
journal = {Neural Networks},
number = {1},
pages = {1017--1024},
title = {{Generating Text with Recurrent Neural Networks}},
url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
volume = {131},
year = {2011}
}
@article{Mikolov2012,
abstract = {Statistical language models are crucial part of many successful applications, such as au- tomatic speech recognition and statistical machine translation (for example well-known Google Translate). Traditional techniques for estimating these models are based on N- gram counts. Despite known weaknesses of N-grams and huge efforts of research commu- nities across many fields (speech recognition, machine translation, neuroscience, artificial intelligence, natural language processing, data compression, psychology etc.), N-grams remained basically the state-of-the-art. The goal of this thesis is to present various archi- tectures of language models that are based on artificial neural networks. Although these models are computationally more expensive than N-gram models, with the presented techniques it is possible to apply them to state-of-the-art systems efficiently. Achieved reductions of word error rate of speech recognition systems are up to 20{\%}, against state- of-the-art N-gram model. The presented recurrent neural network based model achieves the best published performance on well-known Penn Treebank setup. KlÂ´covÂ´},
author = {Mikolov, Tomas},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/thesis.pdf:pdf},
journal = {PhD thesis},
pages = {1--129},
title = {{Statistical Language Models Based on Neural Networks}},
url = {http://www.fit.vutbr.cz/research/pubs/diss.php.cs?id=10158},
year = {2012}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {1045-9227 VO  - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Weiss2003,
archivePrefix = {arXiv},
arxivId = {1106.4557},
author = {Weiss, Gary M and Provost, Foster J},
doi = {10.1613/jair.1199},
eprint = {1106.4557},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/balancing/weiss03a.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {J. Artif. Intell. Res. (JAIR)},
pages = {315--354},
title = {{Learning When Training Data are Costly: The Effect of ClassDistribution on Tree Induction.}},
volume = {19},
year = {2003}
}
