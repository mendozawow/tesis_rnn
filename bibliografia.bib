@article{Barford2007,
abstract = {The continued growth and diversification of the Internet has been accompanied by an increasing prevalence of attacks and intrusions [40]. It can be argued, however, that a significant change in motivation for malicious activity has taken place over the past several years: from vandalism and recognition in the hacker community, to attacks and intrusions for financial gain. This shift has been marked by a growing sophistication in the tools and methods used to conduct attacks, thereby escalating the network security arms race. Our thesis is that the reactive methods for network security that are predominant today are ultimately insufficient and that more proactive methods are required. One such approach is to develop a foundational understanding of the mechanisms employed by malicious software (malware) which is often readily available in source form on the Internet. While it is well known that large IT security companies maintain detailed databases of this information, these are not openly available and we are not aware of any such open repository. In this chapter we begin the process of codifying the capabilities of malware by dissecting four widely-used Internet Relay Chat (IRC) botnet codebases. Each codebase is classified along seven key dimensions including botnet control mechanisms, host control mechanisms, propagation mechanisms, exploits, delivery mechanisms, obfuscation and deception mechanisms. Our study reveals the complexity of botnet software, and we discusses implications for defense strategies based on our analysis.},
author = {Barford, Paul and Yegneswaran, Vinod},
doi = {10.1007/978-0-387-44599-1},
file = {:home/h2o/rnn/tesis/documents/botnets/inside look at botnets.pdf:pdf},
isbn = {978-0-387-32720-4},
issn = {03601315},
journal = {Malware Detection},
pages = {171--191},
pmid = {8514563},
title = {{An inside look at Botnets}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-44599-1},
volume = {27},
year = {2007}
}
@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {1045-9227 VO  - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}
@article{Boden2001,
author = {Bod{\'{e}}n, Mikael},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/10.1.1.16.6652.pdf:pdf},
number = {2},
pages = {1--10},
title = {{A Guide to Recurrent Neural Networks and Backpropagation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.6652},
year = {2001}
}
@article{Chang,
author = {Chang, Wentao},
file = {:home/h2o/rnn/tesis/documents/botnets/measuring botnets in the wild.pdf:pdf},
isbn = {9781450332453},
pages = {3--8},
title = {{Measuring Botnets in the Wild : Some New Trends}}
}
@article{Chawla,
author = {Chawla, Nitesh V},
title = {{DATA MINING FOR IMBALANCED DATASETS : AN OVERVIEW}}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
pmid = {18190633},
title = {{Comparison of balancing techniques for unbalanced datasets}},
volume = {16},
year = {2002}
}
@inproceedings{Donahue2015,
abstract = {Models based on deep convolutional networks have dom- inated recent image interpretation tasks; we investigate whether models which are also recurrent, or “temporally deep”, are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image de- scription and retrieval problems, and video narration chal- lenges. In contrast to current models which assume a ﬁxed spatio-temporal receptive ﬁeld or simple temporal averag- ing for sequential processing, recurrent convolutional mod- els are “doubly deep” in that they can be compositional in spatial and temporal “layers”. Such models may have advantages when target concepts are complex and/or train- ing data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the net- work state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural lan- guage text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual rep- resentations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately deﬁned and/or optimized.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4389v3},
author = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Darrell, Trevor and Saenko, Kate},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298878},
eprint = {arXiv:1411.4389v3},
isbn = {9781467369640},
issn = {10636919},
pages = {2625--2634},
title = {{Long-term recurrent convolutional networks for visual recognition and description}},
volume = {07-12-June-2015},
year = {2015}
}
@article{DzmitryBahdana2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {{Dzmitry Bahdana} and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Iclr 2015},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@article{Garcia2015,
author = {Garc{\'{i}}a, Sebasti{\'{a}}n},
doi = {10.13140/2.1.3488.8006},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/Detecting Botnets by Modeling their Network Behaviors.pdf:pdf},
number = {August},
title = {{Identifying , Modeling and Detecting Botnet Behaviors in the Network U niversidad N acional del C entro de la P rovincia de B uenos A ires D octoral T hesis Identifying , Modeling and Detecting Botnet Behaviors in the Network}},
year = {2015}
}
@article{Garcia2014,
abstract = {Abstract The results of botnet detection methods are usually presented without any comparison. Although it is generally accepted that more comparisons with third-party methods may help to improve the area, few papers could do it. Among the factors that prevent a comparison are the difficulties to share a dataset, the lack of a good dataset, the absence of a proper description of the methods and the lack of a comparison methodology. This paper compares the output of three different botnet detection methods by executing them over a new, real, labeled and large botnet dataset. This dataset includes botnet, normal and background traffic. The results of our two methods (BClus and CAMNEP) and BotHunter were compared using a methodology and a novel error metric designed for botnet detections methods. We conclude that comparing methods indeed helps to better estimate how good the methods are, to improve the algorithms, to build better datasets and to build a comparison methodology.},
author = {Garc{\'{i}}a, Sebasti{\'{a}}n and Grill, Martin and Stiborek, Jan and Zunino, Alejandro},
doi = {http://dx.doi.org/10.1016/j.cose.2014.05.011},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a et al. - 2014 - An Empirical Comparison of Botnet Detection Methods.pdf:pdf},
issn = {0167-4048},
journal = {Computers {\&} Security},
keywords = {Anomaly detection,Botnet dataset,Botnet detection,Malware detection,Methods comparison,Network traffic},
number = {0},
pages = {100 -- 123},
title = {{An Empirical Comparison of Botnet Detection Methods}},
url = {http://www.sciencedirect.com/science/article/pii/S0167404814000923},
volume = {45},
year = {2014}
}
@article{Garcia2014c,
annote = {Articulo que describe el comportamiento de C{\&}C},
author = {Garc{\'{i}}a, Sebasti{\'{a}}n and Uhl{\'{i}}ř, Vojt{\v{e}}ch and Rehak, Martin},
doi = {10.1145/2602945.2602949},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a, Uhl{\'{i}}ř, Rehak - 2014 - Identifying and modeling botnet C{\&}C behaviors.pdf:pdf},
isbn = {9781450327282},
journal = {Proceedings of the 1st International Workshop on Agents and CyberSecurity - ACySE '14},
keywords = {botnet,malware,network behavior,network security},
number = {MAY 2014},
pages = {1--8},
title = {{Identifying and modeling botnet C{\&}C behaviors}},
url = {http://dl.acm.org/citation.cfm?doid=2602945.2602949},
year = {2014}
}
@article{Golik2013,
abstract = {In this paper we investigate the error criteria that are optimized during the training of artificial neural networks (ANN). We compare the bounds of the squared error (SE) and the crossentropy (CE) criteria being the most popular choices in stateof- The art implementations. The evaluation is performed on automatic speech recognition (ASR) and handwriting recognition (HWR) tasks using a hybrid HMM-ANN model. We find that with randomly initialized weights, the squared error based ANN does not converge to a good local optimum. However, with a good initialization by pre-training, the word error rate of our best CE trained system could be reduced from 30.9{\%} to 30.5{\%} on the ASR, and from 22.7{\%} to 21.9{\%} on the HWR task by performing a few additional "fine-tuning" iterations with the SE criterion. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01842v1},
author = {Golik, Pavel and Doetsch, Patrick and Ney, Hermann},
doi = {10.1145/1102351.1102422},
eprint = {arXiv:1503.01842v1},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/11paper.pdf:pdf},
isbn = {1595931805},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Automatic speech recognition,Handwriting recognition,Hybrid approach,Training criterion for ANN training},
number = {2},
pages = {1756--1760},
title = {{Cross-entropy vs. Squared error training: A theoretical and experimental comparison}},
volume = {2},
year = {2013}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
eprint = {arXiv:1308.0850v5},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/1308.0850v5.pdf:pdf},
isbn = {2000201075},
journal = {arXiv preprint arXiv:1308.0850},
pages = {1--43},
title = {{Generating sequences with recurrent neural networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Graves2013a,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {arXiv:1303.5778v1},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Icassp},
number = {3},
pages = {6645--6649},
title = {{Speech Recognition With Deep Recurrent Neural Networks}},
year = {2013}
}
@article{Hachem2011,
abstract = {The new threat of the Internet, but little known to the 'general public' is constituted by botnets. Botnets are networks of infected computers, which are headed by a pirate called also 'Attacker' or 'Master'. The botnets are nowadays mainly responsible for large-scale coordinated attacks. The attacker can ask the infected computers called 'Agents' or 'Zombies' to perform all sorts of tasks for him, like sending spam, performing DDoS attacks, phishing campaigns, delivering malware, or leasing or selling their botnets to other fraudsters anywhere. In this paper we present a classification that reflects the life cycle and current resilience techniques of botnets, distinguishing the propagation, the injection, the control and the attack phases. Then we study the effectiveness of the adopted taxonomy by applying it to existing botnets to study their main characteristics. We conclude by the upcoming steps in our research.},
author = {Hachem, Nabil and {Ben Mustapha}, Yosra and Granadillo, Gustavo Gonzalez and Debar, Herve},
doi = {10.1109/SAR-SSI.2011.5931395},
file = {:home/h2o/rnn/tesis/documents/botnets/Botnets{\_}Lifecycle{\_}and{\_}taxonomy.pdf:pdf},
isbn = {9781457707377},
journal = {2011 Conference on Network and Information Systems Security, SAR-SSI 2011, Proceedings},
number = {March 2016},
title = {{Botnets: Lifecycle and taxonomy}},
year = {2011}
}
@article{Hinton2014,
author = {Hinton, Geoffrey},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/JMLRdropout.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Jozefowicz2015,
author = {Jozefowicz, R},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/jozefowicz15.pdf:pdf},
journal = {Proceedings of the  {\ldots}},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2015{\_}jozefowicz15.pdf},
volume = {37},
year = {2015}
}
@article{Karim2014,
author = {Karim, Ahmad and Salleh, Rosli Bin and Shiraz, Muhammad and Shah, Syed Adeel Ali and Awan, Irfan and Anuar, Nor Badrul},
doi = {10.1631/jzus.C1300242},
file = {:home/h2o/rnn/tesis/documents/detecting{\_}botnets.pdf:pdf},
issn = {1869-1951},
journal = {Journal of Zhejiang University SCIENCE C},
keywords = {10,1631,anomaly detection,attack,botnet detection,c1300242,defense,doi,jzus,network security,taxonomy},
number = {11},
pages = {943--983},
title = {{Botnet detection techniques: review, future trends, and issues}},
url = {http://link.springer.com/10.1631/jzus.C1300242},
volume = {15},
year = {2014}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02078v1},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {arXiv:1506.02078v1},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/1506.02078v2.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv},
pages = {1--13},
title = {{Visualizing and Understanding Recurrent Networks}},
year = {2015}
}
@inproceedings{Karpathy2015a,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Li, Fei Fei},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1412.2306},
isbn = {9781467369640},
issn = {10636919},
pages = {3128--3137},
pmid = {16873662},
title = {{Deep visual-semantic alignments for generating image descriptions}},
volume = {07-12-June-2015},
year = {2015}
}
@article{Livadas2006,
abstract = {To date, techniques to counter cyber-attacks have predominantly been reactive; they focus on monitoring network traffic, detecting anomalies and cyber-attack traffic patterns, and, a posteriori, combating the cyber-attacks and mitigating their effects. Contrary to such approaches, we advocate proactively detecting and identifying botnets prior to their being used as part of a cyber-attack (Strayer et al., 2006). In this paper, we present our work on using machine learning-based classification techniques to identify the command and control (C2) traffic of IRC-based botnets - compromised hosts that are collectively commanded using Internet relay chat (IRC). We split this task into two stages: (I) distinguishing between IRC and non-IRC traffic, and (II) distinguishing between botnet and real IRC traffic. For stage I, we compare the performance of J48, naive Bayes, and Bayesian network classifiers, identify the features that achieve good overall classification accuracy, and determine the classification sensitivity to the training set size. While sensitive to the training data and the attributes used to characterize communication flows, machine learning-based classifiers show promise in identifying IRC traffic. Using classification in stage II is trickier, since accurately labeling IRC traffic as botnet and non-botnet is challenging. We are currently exploring labeling flows as suspicious and non-suspicious based on telltales of hosts being compromised},
author = {Livadas, C. and Walsh, R. and Lapsley, D. and Strayer, W.T.},
doi = {10.1109/LCN.2006.322210},
isbn = {1-4244-0418-5},
issn = {0742-1303},
journal = {Proceedings. 2006 31st IEEE Conference on Local Computer Networks},
title = {{Usilng Machine Learning Technliques to Identify Botnet Traffic}},
year = {2006}
}
@article{Martens2011,
abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely diffi- cult to train them properly. Fortunately, re- cent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence prob- lems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free op- timizer (HF) by applying them to character-level language modeling tasks. The standard RNN ar- chitecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or gated) con- nections which allow the current input charac- ter to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character- level language modeling a hierarchical non- parametric sequence model. To our knowledge this represents the largest recurrent neural net- work application to date.},
author = {Martens, James},
doi = {2},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/LANG-RNN.pdf:pdf},
isbn = {9781450306195},
issn = {1},
journal = {Neural Networks},
number = {1},
pages = {1017--1024},
title = {{Generating Text with Recurrent Neural Networks}},
url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
volume = {131},
year = {2011}
}
@article{Medsker2000,
abstract = {With existent uses ranging from motion detection to music synthesis to financial forecasting, recurrent neural networks have generated widespread attention. The tremendous interest in these networks drives Recurrent Neural Networks: Design and Applications, a summary of the design, applications, current research, and challenges of this subfield of artificial neural networks. This overview incorporates every aspect of recurrent neural networks. It outlines the wide variety of complex learning techniques and associated research projects. Each chapter addresses architectures, from fully connected to partially connected, including recurrent multilayer feedforward. It presents problems involving trajectories, control systems, and robotics, as well as RNN use in chaotic systems. The authors also share their expert knowledge of ideas for alternate designs and advances in theoretical aspects. The dynamical behavior of recurrent neural networks is useful for solving problems in science, engineering, and business. This approach will yield huge advances in the coming years. Recurrent Neural Networks illuminates the opportunities and provides you with a broad view of the current events in this rich field.},
author = {Medsker, L. R. and Jain, L. C.},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/RNN{\_}Design{\_}And{\_}Applications{\_}LR{\_}Medsker.pdf:pdf},
isbn = {9780849371813},
pages = {414},
title = {{Recurrent neural networks: design and applications}},
url = {http://books.google.co.uk/books?id=ME1SAkN0PyMC},
year = {2000}
}
@article{Mikolov2010,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/mikolov{\_}2010.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Mikolov2012,
abstract = {Statistical language models are crucial part of many successful applications, such as au- tomatic speech recognition and statistical machine translation (for example well-known Google Translate). Traditional techniques for estimating these models are based on N- gram counts. Despite known weaknesses of N-grams and huge efforts of research commu- nities across many fields (speech recognition, machine translation, neuroscience, artificial intelligence, natural language processing, data compression, psychology etc.), N-grams remained basically the state-of-the-art. The goal of this thesis is to present various archi- tectures of language models that are based on artificial neural networks. Although these models are computationally more expensive than N-gram models, with the presented techniques it is possible to apply them to state-of-the-art systems efficiently. Achieved reductions of word error rate of speech recognition systems are up to 20{\%}, against state- of-the-art N-gram model. The presented recurrent neural network based model achieves the best published performance on well-known Penn Treebank setup. Kl´cov´},
author = {Mikolov, Tomas},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/thesis.pdf:pdf},
journal = {PhD thesis},
pages = {1--129},
title = {{Statistical Language Models Based on Neural Networks}},
url = {http://www.fit.vutbr.cz/research/pubs/diss.php.cs?id=10158},
year = {2012}
}
@article{Neural2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.2329v5},
author = {Neural, Recurrent and Architectures, Network and Models, State Space and Time, Backpropagation Through and Memory, Associative and Networks, Hopfield and Machines, Boltzmann},
doi = {10.1201/9781420049176},
eprint = {arXiv:1409.2329v5},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/rnn/l12.pdf:pdf},
isbn = {0849371813},
pages = {1--20},
title = {{Recurrent Neural Networks}},
year = {2015}
}
@book{Olson2008,
abstract = {The intent of this book is to describe some recent data mining tools that have proven effective in dealing with data sets which often involve uncer- tain description or other complexities that cause difficulty for the conven- tional approaches of logistic regression, neural network models, and deci- sion trees. Among these traditional algorithms, neural network models often have a relative advantage when data is complex. We will discuss methods with simple examples, review applications, and evaluate relative advantages of several contemporary methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Olson, David L Dl and Delen, Dursun},
doi = {10.1007/978-3-540-76917-0},
eprint = {arXiv:1011.1669v3},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/Advanced-Data-Mining-Techniques.pdf:pdf},
isbn = {9783540769163},
issn = {00093696},
pages = {2008},
pmid = {25246403},
title = {{Advanced data mining techniques}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=2vb-LZEn8uUC{\&}oi=fnd{\&}pg=PA2{\&}dq=Advanced+Data+Mining+Techniques{\&}ots=zV10227KpP{\&}sig=dlD9enPYAgM0VL1repniEvrO-JU{\%}5Cnhttp://www.springerlink.com/index/10.1007/978-3-540-76917-0},
year = {2008}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {Proceedings of The 30th International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}
@article{Ruder2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04747v1},
author = {Ruder, Sebastian},
eprint = {arXiv:1609.04747v1},
file = {:home/h2o/rnn/tesis/tesis{\_}rnn/documentacion/overview{\_}gradient{\_}descent.pdf:pdf},
pages = {1--12},
title = {{An overview of gradient descent optimization}},
year = {2016}
}
@article{Saad2011,
abstract = {Botnets have become one of the major threats on the Internet for serving as a vector for carrying attacks against organizations and committing cybercrimes. They are used to generate spam, carry out DDOS attacks and click-fraud, and steal sensitive information. In this paper, we propose a new approach for characterizing and detecting botnets using network traffic behaviors. Our approach focuses on detecting the bots before they launch their attack. We focus in this paper on detecting P2P bots, which represent the newest and most challenging types of botnets currently available. We study the ability of five different commonly used machine learning techniques to meet online botnet detection requirements, namely adaptability, novelty detection, and early detection. The results of our experimental evaluation based on existing datasets show that it is possible to detect effectively botnets during the botnet Command-and- Control (C{\&}C) phase and before they launch their attacks using traffic behaviors only. However, none of the studied techniques can address all the above requirements at once.},
author = {Saad, Sherif and Traore, Issa and Ghorbani, Ali and Sayed, Bassam and Zhao, David and Lu, Wei and Felix, John and Hakimian, Payman},
doi = {10.1109/PST.2011.5971980},
file = {:home/h2o/rnn/tesis/documents/botnets/detecting botnets through network behavior analysis machine learning.pdf:pdf},
isbn = {9781457705847},
journal = {2011 9th Annual International Conference on Privacy, Security and Trust, PST 2011},
number = {March 2016},
pages = {174--180},
title = {{Detecting P2P botnets through network behavior analysis and machine learning}},
year = {2011}
}
@article{Soltani2014,
author = {Soltani, Somayeh and Seno, SAH},
file = {:home/h2o/rnn/tesis/documents/botnets/survey on real world botnets and detection mechanisms.pdf:pdf},
journal = {International Journal of {\ldots}},
number = {2},
pages = {116--127},
title = {{A survey on real world botnets and detection mechanisms}},
url = {http://iaesjournal.com/online/index.php/IJINS/article/view/6231},
volume = {3},
year = {2014}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {NIPS},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
isbn = {1409.3215},
issn = {09205691},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}
@inproceedings{Vinyals2015,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU score improvements on Flickr30k, from 55 to 66, and on SBU, from 19 to 27.},
archivePrefix = {arXiv},
arxivId = {1411.4555v1},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298935},
eprint = {1411.4555v1},
isbn = {9781467369640},
issn = {10636919},
pages = {3156--3164},
pmid = {903},
title = {{Show and tell: A neural image caption generator}},
volume = {07-12-June-2015},
year = {2015}
}
@article{,
doi = {10.13140/2.1.3488.8006},
file = {:home/h2o/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Identifying , Modeling and Detecting Botnet Behaviors in the Network U niversidad N acional del C entro de la P rovinci.pdf:pdf},
number = {August},
title = {{Identifying , Modeling and Detecting Botnet Behaviors in the Network U niversidad N acional del C entro de la P rovincia de B uenos A ires D octoral T hesis Identifying , Modeling and Detecting Botnet Behaviors in the Network}},
year = {2015}
}
