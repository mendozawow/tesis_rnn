#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language spanish-mexico
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language french
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Marco Teórico
\end_layout

\begin_layout Subsection
Botnets y Malware
\end_layout

\begin_layout Subsubsection
Definición de Botnet
\end_layout

\begin_layout Standard
Se define como Botnet al conjunto de dispositivos infectados por un bot
 que conforman una red controlada por un nodo central, denominado BotMaster,
 quien tiene la capacidad de comunicarse con cada bot y ordenar la ejecución
 de órdenes específicas, tales como scans de las redes asociadas al host
 infectado por el bot, ataques distribuidos de denegación de servicio (DDOS),
 spamming, bitcoin mining, procesamiento distribuido orientado a desencriptación
, entre otras.
\end_layout

\begin_layout Subsubsection
Ciclo de vida
\end_layout

\begin_layout Standard
Las Botnets en general siguen un conjunto de etapas a lo largo de su existencia.
 Cada etapa ofrece un conjunto de alternativas para tratar de poder responder
 ante la amenaza que representan.
 Se pueden definir las siguientes etapas [Botnets: Lifecycle and Taxonomy]:
\end_layout

\begin_layout Enumerate

\series bold
Distribución e infección: 
\series default
En general utilizan enfoques comunes vistos en otros tipos de malware.
 En particular se ha advertido un incremento en el uso de las redes sociales
 como método para llegar a la infección del host, a través del uso de ingeniería
 social.
 Otros métodos comunes de infección los componen el envío de mails con archivos
 adjuntos comprometidos, la explotación de vulnerabilidades en alguna aplicación
 corriendo en el host remoto, la propagación de malware a través de redes
 de compartición de archivos P2P, etc.
 Habiendo infectado, el bot buscará propagarse buscando vulnerabilidades
 en otros hosts adyacentes al infectado.
 
\end_layout

\begin_layout Enumerate

\series bold
Command & Control (C&C): 
\series default
Se denomina canal C&C al conducto al conducto de comunicación mediante el
 cual el Bot Master envía órdenes al bot.
 Si bien existen diferentes modelos y topologías (IRC, HTTP, P2P, etc),
 éstos son invariantes entre los bots y hasta cierto punto ofrecen un comportami
ento parecido entre distintas Botnets.
 En esta etapa el bot establece el método de comunicación con el Master
 y la comunicación propiamente dicha.
\end_layout

\begin_layout Enumerate

\series bold
Ejecución de órdenes: 
\series default
Entre las órdenes más comunes se pueden encontrar: Ataques distribuidos
 de denegación de servicio, spamming, distribución de malware, espionaje,
 phising, 
\end_layout

\begin_layout Enumerate

\series bold
Ofuscación y actualización
\end_layout

\begin_layout Subsubsection

\lang spanish
Tipos
\lang spanish-mexico
 de Botnets
\end_layout

\begin_layout Subsubsection
C&C Channel
\end_layout

\begin_layout Subsubsection
Detection approaches
\end_layout

\begin_layout Subsection
Stratosphere Testing Framework (modelo de comportamiento y modelo de detección)
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Mencionar el problema de los modelos de markov para mirar hacia atrás.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Mencionar como se va el modelo de markov a medida se analiza un mayor numero
 de estados? (limite maxlen)
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Redes Neuronales Recurrentes (RNN)
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Hay que hablar del desbalanceo como uno de los problemas de las RNN
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Una red neuronal recurrente (RNN) es una clase de red neuronal artificial
 donde la red contiene al menos una conexión de retroalimentación, de tal
 manera que las activaciones pueden fluir alrededor en un bucle.
 Esto le da a la red la capacidad para realizar procesamiento temporal y
 a su vez la capacidad para aprender secuencias.
 Esta es una diferencia fundamental al compararlas con redes neuronales
 tradicionales (ej.
 Perceptrón Multicapa), donde tanto las entradas como las salidas se encuentran
 restringidas a vectores de tamaño fijo previamente definidos.
 A continuación se presentan un conjunto de ejemplos para reforzar este
 concepto:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Graphics
	filename rnn_examples.jpeg

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Cada rectángulo es un vector y las flechas representan funciones.
 Los vectores de entrada están representados con rojo, los vectores de salida
 con azul y los vectores verdes mantienen el estado de la red.
 De izquierda a derecha: (1) red tradicional sin RNN, desde una entrada
 fija a una salida fija (ej.
 clasificación de imágenes).(2) Salida secuencial (ej.
 subtitulado de imágenes, toma una imagen como entrada y obtiene como salida
 un conjunto de palabras que la describen).
 (3) Secuencia de entrada (ej.
 análisis de sentimientos, donde una frase es clasificada como una expresión
 sentimental positiva o negativa).
 (4) Secuencia de entrada y secuencia de salida (ej.
 traducción automática, una RNN lee una frase en inglés y emite una frase
 en francés).
 (5) Secuencia sincronizada de entrada y salida (ej.
 clasificación de video donde se quiere etiquetar cada cuadro de video).
 Nótese que en cada caso no hay restricciones preestablecidas en la longitud
 de las secuencias debido a que la transformación recurrente (verde) es
 fija y puede ser aplicada tantas veces como se quiera.
 Como ejemplos concretos de la aplicación exitosa de RNN en la resolución
 de problemas que involucran el análisis de datos secuenciales podemos nombrar
 los trabajos de Mikolov et.
 al.
 (2010) sobre modelado de lenguaje, reconocimiento y generación de escritura
 a mano Graves (2013), traducción automática Sutskever et al.
 (2014); Bahdanau et al.
 (2014), reconocimiento de voz Graves et al.
 (2013), análisis de video Donahue et al.
 (2015) y subtitulado de imágenes Vinyals et al.
 (2015); Karpathy & Fei-Fei (2015).
\end_layout

\begin_layout Standard
Es importante resaltar que no existe gran diferencia entre una implementación
 de RNN y una implementación de una red tradicional.
 De hecho una RNN puede ser convertida a una red tradicional desplegándola
 en el tiempo:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Graphics
	filename rnn/rnn_unfolding.png

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Esto significa que toda la teoría sobre el aprendizaje de la redes tradicionales
 también aplica en cierto sentido a las RNN.
\end_layout

\begin_layout Standard
La arquitectura más simple de una RNN organiza los vectores ocultos de estado
 
\begin_inset Formula $h_{t}^{l}$
\end_inset

en una matriz bidimensional, donde 
\begin_inset Formula $t=1\ldots T$
\end_inset

 es visto como tiempo y 
\begin_inset Formula $l=1\ldots L$
\end_inset

 es visto como profundidad.
 La fila inferior de vectores 
\begin_inset Formula $h_{t}^{0}=x_{t}$
\end_inset

 a profundidad cero contiene los vectores de entrada 
\begin_inset Formula $x_{t}$
\end_inset

y cada vector en la fila superior
\begin_inset Formula $\left\{ h_{t}^{L}\right\} $
\end_inset

se utiliza para predecir un vector de salida 
\begin_inset Formula $y_{t}$
\end_inset

.
 Todos los vectores intermedios 
\begin_inset Formula $h_{t}^{l}$
\end_inset

se computan con una formula de recurrencia basada en 
\begin_inset Formula $h_{t-1}^{l}$
\end_inset

y 
\begin_inset Formula $h_{t}^{l-1}$
\end_inset

.
 A través de estos vectores ocultos, cada salida 
\begin_inset Formula $y_{t}$
\end_inset

 para un tiempo 
\begin_inset Formula $t$
\end_inset

 se convierte en una función de todos los vectores de entrada hasta 
\begin_inset Formula $t,\left\{ x1,\ldots,x_{t}\right\} $
\end_inset

.
 La forma matemática precisa de la recurrencia 
\begin_inset Formula $\left(h_{t-1}^{l},h_{t}^{l-1}\right)\rightarrow h_{t}^{l}$
\end_inset

 varía dependiendo del modelo de implementación.
 Para los objetos de este trabajo se analizará el detalle de implementación
 para una RNN original así como la implementación para el modelo de red
 LSTM.
\end_layout

\begin_layout Standard
La RNN original tiene una recurrencia de la forma
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{t}^{l}=\tanh W^{l}\left(\begin{array}{c}
h_{t}^{l-1}\\
h_{t-1}^{l}
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Donde se asume que todo 
\begin_inset Formula $h\in\mathbb{R^{\eta}}$
\end_inset

.
 La matriz de parámetros en cada capa tiene dimensiones 
\begin_inset Formula $\left[n\,x\,2n\right]$
\end_inset

 y la función
\begin_inset Formula $\tanh$
\end_inset

 es aplicada elemento por elemento.
 Nótese que 
\begin_inset Formula $W^{l}$
\end_inset

 varía entre las capas pero es compartida a través del tiempo.
 Se omiten los vectores de polarización por razones de brevedad.
 Interpretando la ecuación anterior, se puede ver que las entradas de la
 capa anterior en profundidad 
\begin_inset Formula $\left(h_{t}^{l-1}\right)$
\end_inset

 y anterior en tiempo 
\begin_inset Formula $\left(h_{t-1}^{l}\right)$
\end_inset

 son transformadas e interactuan a través de interacción aditiva antes de
 ser procesadas por 
\begin_inset Formula $\tanh$
\end_inset

.
\end_layout

\begin_layout Standard
En cuanto al aprendizaje en la red, esta tarea es llevada a cabo mediante
 el algoritmo de aprendizaje denominado 
\emph on
Backpropagation through time (BPTT o propagación hacia atrás a través del
 tiempo).
 
\emph default
Este algoritmo es una extensión de standard backpropagation que realiza
 descenso de gradiente (más conocido como 
\emph on
gradient descent
\emph default
) en una red completamente desplegada.
 Dada una secuencia de entrenamiento que empieza en un tiempo 
\begin_inset Formula $t_{0}$
\end_inset

 y termina en un tiempo 
\begin_inset Formula $t_{1}$
\end_inset

 , la función de costo total es simplemente la suma en el tiempo del error
 estándar 
\begin_inset Formula $E_{sse/ce}(t)$
\end_inset

 para cada intervalo de tiempo:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{total}(t_{0,}t_{1})=\sum_{t=t_{0}}^{t_{1}}E_{sse/ce}\left(t\right)
\]

\end_inset


\end_layout

\begin_layout Standard
y los cambios de peso de descenso de gradiente tienen contribuciones de
 cada intervalo de tiempo
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\triangle w_{ij}=-\eta\frac{\partial E_{total}\left(t_{0,}t_{1}\right)}{\partial w_{ij}}=-\eta\sum_{t=t_{0}}^{t_{1}}\frac{\partial E_{sse/ce}\left(t\right)}{\partial w_{ij}}
\]

\end_inset


\end_layout

\begin_layout Standard
Las derivadas parciales constituidas 
\begin_inset Formula $\partial E_{sse/ce}\left(t\right)/\partial w_{ij}$
\end_inset

 tienen aportes de las múltiples instancias de cada peso 
\begin_inset Formula $w_{ij}\in\left\{ W_{IH},W_{HH}\right\} $
\end_inset

 y dependen de las entradas y de las activaciones de las unidades ocultas
 en los intervalos de tiempo previos.
 Los errores por tanto ahora han de ser retropropagados a través del tiempo
 así como a través de la red.
\end_layout

\begin_layout Standard
Entre las consideraciones prácticas al momento de entrenar una RNN tradicional,
 existen un problema fundamental que atenta contra la capacidad de aprendizaje
 de la red: las RNN tradicionales tienen grandes dificultades a la hora
 de aprender dependencias a largo plazo (ej.
 dependencias entre intervalos que se encuentran a gran distancia, como
 podría ser en el análisis de secuencias de texto la incidencia que puede
 tener la primer palabra con la última de una misma secuencia).
 Este problema se presenta debido al desvanecimiento del gradiente, y por
 otro lado debido a la explosión del gradiente.
\end_layout

\begin_layout Standard
Este fenómeno se debe parcialmente al hecho de que la información que fluye
 a través de la red es sometida a múltiples etapas de multiplicación.
 Sabiendo que cualquier número multiplicado frecuentemente por un número
 ligeramente mayor a uno puede llegar a ser inmensamente grande.
 Por otro lado, su inversa, esto es, multiplicar por un número menor a uno,
 también es verdadera.
 Debido a que las capas e intervalos de tiempo de la RNN se relacionan entre
 sí mediante multiplicación, las derivadas son susceptibles a desvanecer
 o explotar.
 Para el caso de la explosión del gradiente, los valores que toma se encuentran
 tan saturados en un extremo que terminan generando una incidencia demasiado
 grande.
 Por otro lado, los gradientes desvanecientes pueden volverse demasiado
 pequeños, generando que prácticamente no tengan ninguna incidencia en el
 aprendizaje; en síntesis, la red se vuelve incapaz de aprender dependencias
 a largo plazo.
\end_layout

\begin_layout Standard
Mientras que el problema de la explosión del gradiente se puede resolver
 de manera simple, truncando o aplastando el valor, el problema del desvanecimie
nto del gradiente se vuelve realmente complejo, principalmente porque no
 es evidente cuando ocurre y porque cuando ocurre, resolver el problema
 no es una tarea simple.
 Es por esto que para solventar estas limitaciones se han propuesto nuevos
 modelos de RNN capaces de aprender dependencias a largo plazo, entre los
 cuales se destaca el modelo conocido como LSTM.
\end_layout

\begin_layout Paragraph
Long Short Term Memory (LSTM) 
\end_layout

\begin_layout Paragraph
\begin_inset Note Note
status open

\begin_layout Plain Layout
VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS (Andrew Karpathy) arXiv:1506.020
78v2 [cs.LG] 17 Nov 2015
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Long Short-Term Memory (LSTM) Hochreiter & Schmidhuber (1997) fue diseñada
 con el objetivo de mitigar las dificultades al momento de entrenar RNNs
 Bengio et al.
 (1994).
 En particular, se observó que la dinámica de backpropagation producía que
 los gradientes desaparecieran o explotaran (fenómenos conocidos como 
\emph on
vanishing gradient
\emph default
 y 
\emph on
exploding gradient
\emph default
).
 Se descubrió más tarde que la preocupación por la explosión del gradiente
 podía ser mitigada si se restringía los gradientes a un valor maximo a
 través de heurística Pascanu et al.
 (2012).
 Por otro lado, LSTM fue diseñada para mitigar el problema del desvanecimiento
 del gradiente.
 Además de un vector de estado oculto 
\begin_inset Formula $h_{t}^{l}$
\end_inset

, LSTMs también mantienen un vector de memoria 
\begin_inset Formula $c_{t}^{l}$
\end_inset

.
 En cada intervalo de tiempo la red LSTM puede elegir leer, escribir, o
 reinicializar la celda utilizando mecanismos explícitos de compuerta.
 La forma precisa de la actualización es la siguiente:
\end_layout

\begin_layout Standard
\begin_inset Formula $\left(\begin{array}{c}
i\\
f\\
o\\
g
\end{array}\right)=\left(\begin{array}{c}
sigm\\
sigm\\
sigm\\
tanh
\end{array}\right)W^{l}\left(\begin{array}{c}
h_{t}^{l-1}\\
h_{t-1}^{l}
\end{array}\right)$
\end_inset

 
\begin_inset Formula $\mathllap{}\begin{array}{c}
c_{t}^{l}=f\odot c_{t-1}^{l}+i\odot g\\
h_{t}^{l}=o\odot tanh(c_{t}^{l})
\end{array}$
\end_inset


\end_layout

\begin_layout Standard
Aquí, las funciones sigmoideas sigm y tanh son aplicadas elemento por elemento,
 y 
\begin_inset Formula $W^{l}$
\end_inset

es una matriz 
\begin_inset Formula $\left[4n\,x\,2n\right]$
\end_inset

.
 Los tres vectores 
\begin_inset Formula $i,f,o\in\mathbb{R^{\eta}}$
\end_inset

son considerados como compuertas binarias que controlan si cada celda de
 memoria es actualizada, si es puesta a cero, y si su estado local se revela
 en el vector oculto, respectivamente.
 Las activaciones de estas compuertas se basan en la función sigmoide y
 por tanto se permiten variar suavemente entre cero y uno para mantener
 al modelo diferenciable.
 El vector 
\begin_inset Formula $g\in\mathbb{R^{\eta}}$
\end_inset

oscila entre -1 y 1, y se utiliza para modificar los contenidos de la memoria
 de forma aditiva.
 Esta interacción aditiva es una característica crítica del diseño de LSTM,
 debido a que durante backpropagation una operación de suma simplemente
 distribuye gradientes.
 Esto permite que los gradientes en las células de memoria c fluyan hacia
 atrás a través del tiempo sin interrupción durante largos periodos de tiempo,
 o al menos hasta que el flujo es interrumpido con la interacción multiplicativa
 de una compuerta activa de olvido.
 Por último, ha de observarse que una implementación de LSTM requiere que
 se mantengan dos vectores (
\begin_inset Formula $h_{t}^{l}$
\end_inset

 y 
\begin_inset Formula $c_{t}^{l}$
\end_inset

) en cada punto de la red.
\end_layout

\begin_layout Standard
Evidentemente la mayor ventaja que ofrece la arquitectura de red LSTM es
 la capacidad de unir dependencias que se encuentran separadas por grandes
 intervalos de tiempo.
 Por otro lado se debe resaltar que LSTM es capaz de lidiar con ruido, represent
aciones distribuidas y valores continuos.
 En contraste con autómatas de estados finitos o modelos ocultos de Markov,
 LSTM no requiere una elección a 
\emph on
priori
\emph default
 del número finito de estados; en principio es capaz de lidiar con un número
 ilimitado de estados.
\end_layout

\begin_layout Standard
Otra de las ventajas aparentes de esta arquitectura es la falta de necesidad
 de un ajuste fino de parámetros.
 LSTM trabaja bien sobre un amplio rango de parámetros como el ratio de
 aprendizaje (
\emph on
learning rate
\emph default
), polarización de la compuerta de entrada (
\emph on
input gate bias
\emph default
) y polarización de la compuerta de salida (
\emph on
output gate bias
\emph default
).
\end_layout

\begin_layout Standard
En síntesis, se puede concluir que frente a un problema de análisis de secuencia
s donde exista un interés latente de analizar las relaciones entre los component
es de las secuencias, la utilización de una red LSTM se puede considerar
 como una alternativa viable, probada y eficiente.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
LSTM, definición, consideraciones de diseño, soluciones a los problemas
 inherentes a las RNN
\end_layout

\end_inset


\end_layout

\end_body
\end_document
